---
title: "Beta Distribution"
author: "Nasreen"
date: "2025-01-26"
categories: [Machine Learning]      
# image: "image.jpg"
toc: true
toc-depth: 3
jupyter: python3
execute: 
  enabled: true
---

# Introduction

The **Beta Distribution** is a family of continuous probability distribution defined on the interval $[0,1]$ in terms of two positive parameters , denoted by $\alpha$ and $\beta$, that appear as exponents of the variable and its complement to 1,respectively, and control the shape of the distribution.

## Probability Density Function 

The *probability density function*(PDF) of the beta distribution, for $0 \leq x$ or $ 0 <x <1$, and shape parameters $\alpha ,\beta >0$, is a power function of the variable $x$ and of its reflection $(1-x)$ as follows :

$$
\begin{align*}
f(x;\alpha,\beta) &= \text{constant } \cdot x^{\alpha-1} (1-x)^{\beta-1}\\
&= \frac{x^{\alpha-1}(1-x)^{\beta -1}}{\int_0^1 u^{\alpha-1}(1-u)^{\beta-1}du} \\
&=\frac{\gamma(\alpha+\beta)}{\gamma(\alpha)\gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}\\
&= \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}
\end{align*}
$$ {#eq-pdf_beta}

where $\gamma(z)$, is the gamma function. The beta function,$B$, is a normalizing constant to ensure that the total probability is $1$. In the above equation, $x$ is a realization - an observed value that actually occured - of a random variable. 


::: {#thm-my-theorem}
### Pythagoras

This is theorem
:::

*Proof*.


In @thm-my-theorem, we had seen that

::: {#prp-my-proposition}

### My prop

:::

*Proof.*

```{python}
import numpy as np

print("Hello")
```

